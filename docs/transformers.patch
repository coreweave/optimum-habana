diff --git a/src/transformers/models/bridgetower/modeling_bridgetower.py b/src/transformers/models/bridgetower/modeling_bridgetower.py
index ce569157b..5877d3f16 100644
--- a/src/transformers/models/bridgetower/modeling_bridgetower.py
+++ b/src/transformers/models/bridgetower/modeling_bridgetower.py
@@ -23,6 +23,7 @@ import torch
 import torch.utils.checkpoint
 from torch import nn
 from torch.nn import CrossEntropyLoss
+from transformer_engine import pytorch as te

 from ...activations import ACT2FN, QuickGELUActivation
 from ...modeling_outputs import (
@@ -193,7 +194,7 @@ class BridgeTowerResidualAttention(nn.Module):
         super().__init__()

         self.attn = nn.MultiheadAttention(config.hidden_size, config.hidden_size // 64)
-        self.ln_1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.ln_1 = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.mlp = nn.ModuleDict(
             OrderedDict(
                 [
@@ -203,7 +204,7 @@ class BridgeTowerResidualAttention(nn.Module):
                 ]
             )
         )
-        self.ln_2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.ln_2 = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.attn_mask = None

     def attention(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor):
@@ -299,13 +300,13 @@ class BridgeTowerVisionTransformer(nn.Module):
         super().__init__()

         self.embeddings = BridgeTowerVisionEmbeddings(config)
-        self.ln_pre = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.ln_pre = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.transformer = BridgeTowerTransformer(config)
-        self.ln_post = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.ln_post = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.share_layernorm = config.share_layernorm
         if not config.share_layernorm:
             self.ln_separate = nn.ModuleList(
-                [nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) for _ in range(config.num_hidden_layers)]
+                [te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) for _ in range(config.num_hidden_layers)]
             )

     def forward(self, pixel_values: torch.Tensor, attention_mask):
@@ -353,7 +354,7 @@ class BridgeTowerLinkTower(nn.Module):
                 self.scaled_factor = nn.Parameter(torch.tensor(1.0))
             elif config.link_tower_type == "interpolate":
                 self.beta = nn.Parameter(torch.tensor(0.5))
-            self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)
+            self.LayerNorm = te.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)
         else:
             raise NotImplementedError(f"link_tower_type {config.link_tower_type} is not implemented")

@@ -373,7 +374,7 @@ class BridgeTowerSelfOutput(nn.Module):
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.LayerNorm = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.dropout = nn.Dropout(config.hidden_dropout_prob)

     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
@@ -404,7 +405,7 @@ class BridgeTowerOutput(nn.Module):
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.LayerNorm = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.dropout = nn.Dropout(config.hidden_dropout_prob)

     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
@@ -877,7 +878,7 @@ class BridgeTowerTextEmbeddings(nn.Module):

         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.LayerNorm = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.dropout = nn.Dropout(config.hidden_dropout_prob)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
@@ -1000,7 +1001,7 @@ class BridgeTowerPreTrainedModel(PreTrainedModel):
             )
         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.Embedding)):
             module.weight.data.normal_(mean=0.0, std=0.05 * self.config.initializer_factor)
-        elif isinstance(module, nn.LayerNorm):
+        elif isinstance(module, te.LayerNorm):
             module.bias.data.zero_()
             module.weight.data.fill_(1.0)

@@ -1245,8 +1246,8 @@ class BridgeTowerModel(BridgeTowerPreTrainedModel):
         self.cross_modal_text_pooler = BridgeTowerPooler(config)

         # Initialize BridgeTower Components
-        self.cross_modal_text_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.cross_modal_image_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.cross_modal_text_layernorm = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.cross_modal_image_layernorm = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

         if config.share_link_tower_layers:
             self.cross_modal_text_link_tower = BridgeTowerLinkTower(config)
@@ -1508,7 +1509,7 @@ class BridgeTowerPredictionHeadTransform(nn.Module):
             self.transform_act_fn = ACT2FN[config.hidden_act]
         else:
             self.transform_act_fn = config.hidden_act
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.LayerNorm = te.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

     def forward(self, hidden_states):
         hidden_states = self.dense(hidden_states)
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index b9e103761..ea79cd446 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -1139,8 +1139,6 @@ class Trainer:
             optimizer_cls = torch.optim.SGD
         elif args.optim == OptimizerNames.ADAGRAD:
             optimizer_cls = torch.optim.Adagrad
-        elif args.optim == OptimizerNames.RMSPROP:
-            optimizer_cls = torch.optim.RMSprop
         else:
             raise ValueError(f"Trainer cannot instantiate unsupported optimizer: {args.optim}")
         return optimizer_cls, optimizer_kwargs
@@ -1840,6 +1838,11 @@ class Trainer:
                     sampler = sampler if sampler is not None else []
                     _ = list(sampler)

+        # get a batch of data
+        for step, inputs in enumerate(train_dataloader):
+            saved_inputs = inputs
+            break
+
         total_batched_samples = 0
         for epoch in range(epochs_trained, num_train_epochs):
             epoch_iterator = train_dataloader
@@ -1867,7 +1870,14 @@ class Trainer:
                 rng_to_sync = True

             step = -1
-            for step, inputs in enumerate(epoch_iterator):
+            skip_loader = os.getenv("SKIP_LOADER") == '1'
+            # only use one batch of data when skip loader
+            if skip_loader:
+                epoch_iterator_override = [saved_inputs for i in range(len(epoch_iterator))]
+            else:
+                epoch_iterator_override = epoch_iterator
+
+            for step, inputs in enumerate(epoch_iterator_override):
                 total_batched_samples += 1
                 if rng_to_sync:
                     self._load_rng_state(resume_from_checkpoint)
